#cloud-config

---
coreos:
  units:
  - name: etcd3.service
    command: start
    enable: true
    content: |
      [Unit]
      Description=etcd3 service
      Wants=network-online.target
      After=network-online.target

      [Service]
      Restart=on-failure
      RestartSec=5s
      TimeoutSec=3m
      ExecStartPre=/usr/bin/docker build -t etcd3 /home/core/share/etcd3      
      ExecStartPre=/usr/bin/mkdir -p /opt/etcd/data
      ExecStartPre=-/usr/bin/docker rm -f etcd
      ExecStart=/usr/bin/docker run \
      --net=host \
      --name etcd \
      --env ETCD_ADVERTISE_CLIENT_URLS=http://$private_ipv4:2379 \
      --env ETCD_DISCOVERY=https://discovery.etcd.io/00a1c1f456d383dba1a2b542fa3c6866 \
      --env ETCD_INITIAL_ADVERTISE_PEER_URLS=http://$private_ipv4:2380 \
      --env ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379,http://0.0.0.0:4001 \
      --env ETCD_LISTEN_PEER_URLS=http://$private_ipv4:2380,http://$private_ipv4:7001 \
      --env ETCD_DATA_DIR=/opt/etcd/data \
      --volume /opt/etcd/data:/opt/etcd/data \
      etcd3
  - name: generate-certificates.service
    command: start
    enable: true
    content: |
      [Unit]
      Description=Generate kubernetes ceritificates service

      [Service]
      Type=forking
      RemainAfterExit=yes
      Restart=on-failure
      RestartSec=5s
      TimeoutSec=3m
      ExecStartPre=/usr/bin/mkdir -p /opt/bin/
      ExecStartPre=/usr/bin/cp /home/core/share/certificates/ca.pem /etc/kubernetes/ssl/ca.pem
      ExecStartPre=/usr/bin/cp /home/core/share/certificates/ca-key.pem /etc/kubernetes/ssl/ca-key.pem
      ExecStartPre=/usr/bin/cp /home/core/share/certificates/service-account.pem /etc/kubernetes/ssl/service-account.pem
      ExecStartPre=/usr/bin/cp /home/core/share/certificates/service-account-key.pem /etc/kubernetes/ssl/service-account-key.pem
      ExecStartPre=/usr/bin/cp /home/core/share/certificates/admin.pem /etc/kubernetes/ssl/admin.pem
      ExecStart=/usr/bin/cp /home/core/share/certificates/admin-key.pem /etc/kubernetes/ssl/admin-key.pem
      ExecStartPre=/usr/bin/openssl genrsa -out /etc/kubernetes/ssl/apiserver-key.pem 2048
      ExecStartPre=/usr/bin/openssl req -new -key /etc/kubernetes/ssl/apiserver-key.pem -out /etc/kubernetes/ssl/apiserver.csr -subj "/CN=kube-apiserver" -config /etc/kubernetes/ssl/openssl.conf
      ExecStartPre=/usr/bin/openssl x509 -req -in /etc/kubernetes/ssl/apiserver.csr -CA /etc/kubernetes/ssl/ca.pem -CAkey /etc/kubernetes/ssl/ca-key.pem -CAcreateserial -out /etc/kubernetes/ssl/apiserver.pem -days 365 -extensions v3_req -extfile /etc/kubernetes/ssl/openssl.conf
  - name: openvswitch.service
    command: start
    enable: true
    content: |
      [Unit]
      Description=OpenvSwitch Service
      Wants=network.target docker.service
      After=network.target docker.service generate-certificates.service

      [Service]
      RemainAfterExit=yes
      Restart=on-failure
      RestartSec=5s
      TimeoutSec=3m
      ExecStartPre=/opt/custom-env.sh
      ExecStartPre=/sbin/modprobe openvswitch
      ExecStartPre=/sbin/modprobe geneve
      ExecStartPre=/usr/bin/docker build -t openvswitch /home/core/share/openvswitch/
      ExecStart=/usr/bin/docker run \
        --tty \
        --name openvswitch \
        --privileged \
        --net=host \
        --cap-add=NET_ADMIN \
        --volume /opt/sdn/conf:/etc/openvswitch \
        --volume /opt/sdn/run:/var/run/openvswitch \
        --volume /opt/cni:/etc/cni/net.d \
        openvswitch
      ExecStop=-/usr/bin/docker stop openvswitch

      [Install]
      WantedBy=multi-user.target
  - name: overlay.service
    command: start
    enable: true
    content: |
      [Unit]
      Description=Overlay Service
      Wants=network.target docker.service openvswitch.service
      After=network.target docker.service openvswitch.service

      [Service]
      EnvironmentFile=/opt/environment
      RemainAfterExit=yes
      Restart=on-failure
      RestartSec=30s
      TimeoutStartSec=3m
      TimeoutStopSec=3s
      ExecStartPre=/sbin/modprobe openvswitch
      ExecStartPre=/sbin/modprobe geneve
      ExecStartPre=/usr/bin/docker exec openvswitch ovs-vsctl set Open_vSwitch . external_ids:ovn-remote="tcp:$private_ipv4:6642" external_ids:ovn-nb="tcp:$private_ipv4:6641" external_ids:ovn-encap-ip=$private_ipv4 external_ids:ovn-encap-type="geneve"
      ExecStartPre=/usr/bin/docker exec openvswitch ovs-vsctl set Open_vSwitch . external_ids:k8s-api-server="https://$private_ipv4"
      ExecStartPre=/usr/bin/docker exec openvswitch ovs-vsctl set Open_vSwitch . external_ids:k8s-api-token="${API_TOKEN}"
      ExecStartPre=/usr/bin/docker exec openvswitch /usr/share/openvswitch/scripts/ovn-ctl start_northd
      ExecStartPre=/usr/bin/docker exec openvswitch /usr/share/openvswitch/scripts/ovn-ctl start_controller
      ExecStartPre=/usr/bin/docker exec openvswitch ovn-k8s-overlay master-init --cluster-ip-subnet="${POD_NETWORK}" --master-switch-subnet="${POD_SUBNET}" --node-name="$private_ipv4"
      ExecStartPre=/usr/bin/docker exec openvswitch ovn-k8s-overlay minion-init --cluster-ip-subnet="${POD_NETWORK}" --minion-switch-subnet="${POD_SUBNET}" --node-name="$private_ipv4"
      ExecStartPre=/usr/bin/docker exec openvswitch ovs-vsctl add-br br-eth1
      ExecStartPre=/usr/bin/docker exec openvswitch ovs-vsctl add-port br-eth1 eth1
      ExecStartPre=/bin/ip addr flush dev eth1
      ExecStartPre=/bin/ifconfig br-eth1 $private_ipv4/24 up
      ExecStartPre=-/usr/bin/docker exec openvswitch ovn-k8s-overlay gateway-init --cluster-ip-subnet="${POD_SUBNET}" --bridge-interface br-eth1  --physical-ip $private_ipv4/24 --node-name="$private_ipv4" --default-gw ${ETH_GW}
      ExecStartPre=-/usr/bin/docker exec openvswitch ovn-k8s-gateway-helper --physical-bridge=br-eth1 --physical-interface=eth1 --pidfile --detach
      ExecStart=/usr/bin/docker exec openvswitch ovs-appctl vlog/set ANY:ANY:info
  - name: kubectl.service
    command: start
    enable: true
    content: |
      [Unit]
      Description=kubectl download service

      [Service]
      Type=forking
      RemainAfterExit=yes
      Restart=on-failure
      RestartSec=5s
      TimeoutSec=3m
      Environment="USER=root"
      ExecStartPre=/usr/bin/curl -O https://storage.googleapis.com/kubernetes-release/release/v1.6.0/bin/linux/amd64/kubectl
      ExecStartPre=/usr/bin/chmod +x kubectl
      ExecStart=/usr/bin/mv kubectl /opt/bin/kubectl

      [Install]
      RequiredBy=multi-user.target
  - name: kubernetes.service
    command: start
    enable: true
    content: |
      [Unit]
      Description=Kubernetes service
      Wants=network-online.target
      After=network-online.target

      [Service]
      Restart=on-failure
      RestartSec=10
      TimeoutStartSec=10m
      TimeoutStopSec=10s
      ExecStartPre=/usr/bin/mkdir -p /etc/kubernetes/manifests
      ExecStartPre=/usr/bin/docker build -t kubernetes /home/core/share/kubernetes/
      ExecStartPre=/bin/cp /home/core/share/certificates/ca.pem /opt/sdn/conf/k8s-ca.crt
      ExecStartPre=-/usr/bin/docker rm -f kubernetes
      ExecStart=/usr/bin/docker run \
          --tty \
          --net=host \
          --pid=host \
          --privileged \
          --restart=unless-stopped \
          --name kubernetes \
          --volume /etc/kubernetes:/etc/kubernetes \
          --volume /sys:/sys:rw \
          --volume /var/run:/var/run:rw \
          --volume /run:/run:rw \
          --volume /var/lib/docker:/var/lib/docker:rw \
          --volume /var/lib/kubelet:/var/lib/kubelet:shared \
          --volume /var/run/docker.sock:/var/run/docker.sock \
          --volume /var/log/containers:/var/log/containers:rw \
          --volume /opt/cni:/etc/cni/net.d \
          --volume /opt/sdn/conf:/etc/openvswitch \
          --volume /opt/sdn/run:/var/run/openvswitch \
          kubernetes \
          /hyperkube kubelet \
            --allow-privileged \
            --node-ip=$private_ipv4 \
            --kubeconfig=/var/lib/kubelet/kubeconfig \
            --require-kubeconfig \
            --cni-bin-dir /opt/cni/bin \
            --pod-manifest-path=/etc/kubernetes/manifests \
            --cluster-dns=10.3.0.10 \
            --cluster-domain=cluster.local \
            --network-plugin=cni \
            --network-plugin-dir=/etc/cni/net.d \
            --hostname-override=$private_ipv4 \
            --v=2
  - name: watcher.service
    command: start
    enable: true
    content: |
      [Unit]
      Description=Watcher Service
      Wants=network.target docker.service
      After=network.target docker.service generate-certificates.service

      [Service]
      RemainAfterExit=yes
      Restart=on-failure
      RestartSec=5s
      TimeoutSec=3m
      ExecStartPre=/usr/bin/docker build -t watcher /home/core/share/watcher/
      ExecStartPre=-/usr/bin/docker rm -f watcher
      ExecStart=/usr/bin/docker run \
        --tty \
        --name watcher \
        --privileged \
        --net=host \
        --cap-add=NET_ADMIN \
        --volume /opt/sdn/conf:/etc/openvswitch \
        --volume /opt/sdn/run:/var/run/openvswitch \
        watcher
      ExecStop=-/usr/bin/docker stop watcher

      [Install]
      WantedBy=multi-user.target
write_files:
- path: "/var/lib/kubelet/kubeconfig"
  permissions: '0644'
  owner: root
  content: |
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority: /etc/kubernetes/ssl/ca.pem
        server: https://$private_ipv4
      name: default-cluster
    contexts:
    - context:
        cluster: default-cluster
        user: default-admin
      name: default-system
    current-context: default-system
    kind: Config
    preferences: {}
    users:
    - name: default-admin
      user:
        client-certificate: /etc/kubernetes/ssl/admin.pem
        client-key: /etc/kubernetes/ssl/admin-key.pem
- path: "/etc/kubernetes/manifests/kube-apiserver.yml"
  permissions: '0644'
  owner: root
  content: |
    apiVersion: v1
    clusters:
    - cluster:
        certificate-authority: /etc/kubernetes/ssl/ca.pem
        server: https://$private_ipv4
      name: default-cluster
    contexts:
    - context:
        cluster: default-cluster
        user: default-admin
      name: default-system
    current-context: default-system
    kind: Config
    preferences: {}
    users:
    - name: default-admin
      user:
        client-certificate: /etc/kubernetes/ssl/admin.pem
        client-key: /etc/kubernetes/ssl/admin-key.pem
- path: "/etc/kubernetes/manifests/kube-apiserver.yml"
  permissions: '0644'
  owner: root
  content: |
    apiVersion: v1
    kind: Pod
    metadata:
      name: kube-apiserver
      namespace: kube-system
    spec:
      hostNetwork: true
      containers:
      - name: kube-apiserver
        image: kubernetes
        imagePullPolicy: Never
        command:
        - /hyperkube
        - apiserver
        - --bind-address=0.0.0.0
        - --etcd-servers=http://$private_ipv4:2379
        - --allow-privileged=true
        - --service-cluster-ip-range=10.3.0.0/16
        - --secure-port=443
        - --advertise-address=$private_ipv4
        - --admission-control=NamespaceLifecycle,LimitRanger,ServiceAccount,ResourceQuota
        - --tls-cert-file=/etc/kubernetes/ssl/apiserver.pem
        - --tls-private-key-file=/etc/kubernetes/ssl/apiserver-key.pem
        - --client-ca-file=/etc/kubernetes/ssl/ca.pem
        - --service-account-key-file=/etc/kubernetes/ssl/service-account.pem
        - --token-auth-file=/etc/kubernetes/auth/token.csv
        - --apiserver-count=5
        - --runtime-config=extensions/v1beta1=true,extensions/v1beta1/networkpolicies=true
        ports:
        - name: https
          containerPort: 443
          hostPort: 443
        - name: local
          containerPort: 8080
          hostPort: 8080
        volumeMounts:
        - name: ssl-certs-kubernetes
          mountPath: /etc/kubernetes/ssl
          readOnly: true
        - name: ssl-certs-host
          mountPath: /etc/ssl/certs
          readOnly: true
        - name: auth-tokens
          mountPath: /etc/kubernetes/auth
          readOnly: true
        - name: hosts
          mountPath: /etc/hosts
          readOnly: true
      volumes:
      - name: ssl-certs-kubernetes
        hostPath:
          path: /etc/kubernetes/ssl
      - name: ssl-certs-host
        hostPath:
          path: /usr/share/ca-certificates
      - name: auth-tokens
        hostPath:
          path: /etc/kubernetes/auth
      - name: hosts
        hostPath:
          path: /etc/hosts
- path: "/etc/kubernetes/manifests/kube-controller-manager.yml"
  permissions: '0644'
  owner: root
  content: |
    apiVersion: v1
    kind: Pod
    metadata:
      name: kube-controller-manager
      namespace: kube-system
    spec:
      hostNetwork: true
      containers:
      - name: kube-controller-manager
        image: kubernetes
        imagePullPolicy: Never
        command:
        - /hyperkube
        - controller-manager
        - --master=http://127.0.0.1:8080
        - --leader-elect=true
        - --leader-elect-lease-duration=15s
        - --leader-elect-renew-deadline=10s
        - --leader-elect-retry-period=2s
        - --node-monitor-grace-period=40s
        - --node-monitor-period=5s
        - --service-account-private-key-file=/etc/kubernetes/ssl/service-account-key.pem
        - --root-ca-file=/etc/kubernetes/ssl/ca.pem
        - --leader-elect=true
        - --leader-elect-lease-duration=15s
        - --leader-elect-renew-deadline=10s
        - --leader-elect-retry-period=2s
        - --node-monitor-grace-period=40s
        - --node-monitor-period=5s
        livenessProbe:
          httpGet:
            host: 127.0.0.1
            path: /healthz
            port: 10252
          initialDelaySeconds: 15
          timeoutSeconds: 1
        volumeMounts:
        - name: ssl-certs-kubernetes
          mountPath: /etc/kubernetes/ssl
          readOnly: true
        - name: ssl-certs-host
          mountPath: /etc/ssl/certs
          readOnly: true
      volumes:
      - name: ssl-certs-kubernetes
        hostPath:
          path: /etc/kubernetes/ssl
      - name: ssl-certs-host
        hostPath:
          path: /usr/share/ca-certificates
- path: "/etc/kubernetes/manifests/kube-proxy.yml"
  permissions: '0644'
  owner: root
  content: |
    apiVersion: v1
    kind: Pod
    metadata:
      name: kube-proxy
      namespace: kube-system
    spec:
      hostNetwork: true
      containers:
      - name: kube-proxy
        image: kubernetes
        imagePullPolicy: Never
        command:
        - /hyperkube
        - proxy
        - --master=http://127.0.0.1:8080
        - --proxy-mode=iptables
        securityContext:
          privileged: true
        volumeMounts:
        - name: ssl-certs-host
          mountPath: /etc/ssl/certs
          readOnly: true
      volumes:
      - name: ssl-certs-host
        hostPath:
          path: /usr/share/ca-certificates
- path: "/etc/kubernetes/manifests/kube-scheduler.yml"
  permissions: '0644'
  owner: root
  content: |
    apiVersion: v1
    kind: Pod
    metadata:
      name: kube-scheduler
      namespace: kube-system
    spec:
      hostNetwork: true
      containers:
      - name: kube-scheduler
        image: kubernetes
        imagePullPolicy: Never
        command:
        - /hyperkube
        - scheduler
        - --master=http://127.0.0.1:8080
        - --leader-elect=true
        - --leader-elect-lease-duration=15s
        - --leader-elect-renew-deadline=10s
        - --leader-elect-retry-period=2s
        livenessProbe:
          httpGet:
            host: 127.0.0.1
            path: /healthz
            port: 10251
          initialDelaySeconds: 15
          timeoutSeconds: 1
- path: "/etc/kubernetes/ssl/openssl.conf"
  permissions: '0644'
  owner: root
  content: |
    [req]
    req_extensions = v3_req
    distinguished_name = req_distinguished_name
    [req_distinguished_name]
    [ v3_req ]
    basicConstraints = CA:FALSE
    keyUsage = nonRepudiation, digitalSignature, keyEncipherment
    subjectAltName = @alt_names
    [alt_names]
    DNS.1 = kubernetes
    DNS.2 = kubernetes.default
    DNS.3 = kubernetes.default.svc
    DNS.4 = kubernetes.default.svc.cluster.local
    IP.1 = 10.3.0.1
    IP.2 = $private_ipv4
- path: "/etc/kubernetes/manifests/kube-dashboard.yml"
  permissions: '0644'
  owner: root
  content: |
    kind: Deployment
    apiVersion: extensions/v1beta1
    metadata:
      labels:
        app: kubernetes-dashboard
      name: kubernetes-dashboard
      namespace: kube-system
    spec:
      replicas: 1
      revisionHistoryLimit: 10
      selector:
        matchLabels:
          app: kubernetes-dashboard
      template:
        metadata:
          labels:
            app: kubernetes-dashboard
          # Comment the following annotation if Dashboard must not be deployed on master
          annotations:
            scheduler.alpha.kubernetes.io/tolerations: |
              [
                {
                  "key": "dedicated",
                  "operator": "Equal",
                  "value": "master",
                  "effect": "NoSchedule"
                }
              ]
        spec:
          containers:
          - name: kubernetes-dashboard
            image: gcr.io/google_containers/kubernetes-dashboard-amd64:v1.6.0
            imagePullPolicy: Always
            ports:
            - containerPort: 9090
              protocol: TCP
            args:
              # Uncomment the following line to manually specify Kubernetes API server Host
              # If not specified, Dashboard will attempt to auto discover the API server and connect
              # to it. Uncomment only if the default does not work.
              # - --apiserver-host=http://my-address:port
            livenessProbe:
              httpGet:
                path: /
                port: 9090
              initialDelaySeconds: 30
              timeoutSeconds: 30
    ---
    kind: Service
    apiVersion: v1
    metadata:
      labels:
        app: kubernetes-dashboard
      name: kubernetes-dashboard
      namespace: kube-system
    spec:
      type: NodePort
      ports:
      - port: 80
        targetPort: 9090
      selector:
        app: kubernetes-dashboard
- path: "/etc/kubernetes/manifests/kube-dns.yml"
  permissions: '0644'
  owner: root
  content: |
    apiVersion: v1
    kind: ServiceAccount
    metadata:
      name: kube-dns
      namespace: kube-system
      labels:
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
    ---
    apiVersion: v1
    kind: ConfigMap
    metadata:
      name: kube-dns
      namespace: kube-system
      labels:
        addonmanager.kubernetes.io/mode: EnsureExists
    ---
    apiVersion: extensions/v1beta1
    kind: Deployment
    metadata:
      name: kube-dns
      namespace: kube-system
      labels:
        k8s-app: kube-dns
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
    spec:
      # replicas: not specified here:
      # 1. In order to make Addon Manager do not reconcile this replicas parameter.
      # 2. Default is 1.
      # 3. Will be tuned in real time if DNS horizontal auto-scaling is turned on.
      strategy:
        rollingUpdate:
          maxSurge: 10%
          maxUnavailable: 0
      selector:
        matchLabels:
          k8s-app: kube-dns
      template:
        metadata:
          labels:
            k8s-app: kube-dns
          annotations:
            scheduler.alpha.kubernetes.io/critical-pod: ''
        spec:
          imagePullSecrets:
          - name: quay.io-kube
          tolerations:
          - key: "CriticalAddonsOnly"
            operator: "Exists"
          volumes:
          - name: kube-dns-config
            configMap:
              name: kube-dns
              optional: true
          containers:
          - name: kubedns
            image: gcr.io/google_containers/k8s-dns-kube-dns-amd64:1.14.1
            resources:
              # TODO: Set memory limits when we've profiled the container for large
              # clusters, then set request = limit to keep this container in
              # guaranteed class. Currently, this container falls into the
              # "burstable" category so the kubelet doesn't backoff from restarting it.
              limits:
                memory: 170Mi
              requests:
                cpu: 100m
                memory: 70Mi
            livenessProbe:
              httpGet:
                path: /healthcheck/kubedns
                port: 10054
                scheme: HTTP
              initialDelaySeconds: 60
              timeoutSeconds: 5
              successThreshold: 1
              failureThreshold: 5
            readinessProbe:
              httpGet:
                path: /readiness
                port: 8081
                scheme: HTTP
              # we poll on pod startup for the Kubernetes master service and
              # only setup the /readiness HTTP server once that's available.
              initialDelaySeconds: 3
              timeoutSeconds: 5
            args:
            - --domain=cluster.local.
            - --dns-port=10053
            - --config-dir=/kube-dns-config
            - --v=2
            env:
            - name: PROMETHEUS_PORT
              value: "10055"
            ports:
            - containerPort: 10053
              name: dns-local
              protocol: UDP
            - containerPort: 10053
              name: dns-tcp-local
              protocol: TCP
            - containerPort: 10055
              name: metrics
              protocol: TCP
            volumeMounts:
            - name: kube-dns-config
              mountPath: /kube-dns-config
          - name: dnsmasq
            image: gcr.io/google_containers/k8s-dns-dnsmasq-nanny-amd64:1.14.1
            livenessProbe:
              httpGet:
                path: /healthcheck/dnsmasq
                port: 10054
                scheme: HTTP
              initialDelaySeconds: 60
              timeoutSeconds: 5
              successThreshold: 1
              failureThreshold: 5
            args:
            - -v=2
            - -logtostderr
            - -configDir=/etc/k8s/dns/dnsmasq-nanny
            - -restartDnsmasq=true
            - --
            - -k
            - --cache-size=1000
            - --log-facility=-
            - --server=/cluster.local/127.0.0.1#10053
            - --server=/in-addr.arpa/127.0.0.1#10053
            - --server=/ip6.arpa/127.0.0.1#10053
            ports:
            - containerPort: 53
              name: dns
              protocol: UDP
            - containerPort: 53
              name: dns-tcp
              protocol: TCP
            # see: https://github.com/kubernetes/kubernetes/issues/29055 for details
            resources:
              requests:
                cpu: 150m
                memory: 20Mi
            volumeMounts:
            - name: kube-dns-config
              mountPath: /etc/k8s/dns/dnsmasq-nanny
          - name: sidecar
            image: gcr.io/google_containers/k8s-dns-sidecar-amd64:1.14.1
            livenessProbe:
              httpGet:
                path: /metrics
                port: 10054
                scheme: HTTP
              initialDelaySeconds: 60
              timeoutSeconds: 5
              successThreshold: 1
              failureThreshold: 5
            args:
            - --v=2
            - --logtostderr
            - --probe=kubedns,127.0.0.1:10053,kubernetes.default.svc.cluster.local,5,A
            - --probe=dnsmasq,127.0.0.1:53,kubernetes.default.svc.cluster.local,5,A
            ports:
            - containerPort: 10054
              name: metrics
              protocol: TCP
            resources:
              requests:
                memory: 20Mi
                cpu: 10m
          dnsPolicy: Default  # Don't use cluster DNS.
          serviceAccountName: kube-dns
    ---
    apiVersion: v1
    kind: Service
    metadata:
      name: kube-dns
      namespace: kube-system
      labels:
        k8s-app: kube-dns
        kubernetes.io/cluster-service: "true"
        addonmanager.kubernetes.io/mode: Reconcile
        kubernetes.io/name: "KubeDNS"
    spec:
      selector:
        k8s-app: kube-dns
      clusterIP: 10.3.0.10
      ports:
      - name: dns
        port: 53
        protocol: UDP
      - name: dns-tcp
        port: 53
        protocol: TCP
- path: "/etc/kubernetes/auth/token.csv"
  permissions: '0644'
  owner: root
  content: |
    MXhAQm29G3TLu5NBcumwQvKHyNkZ6XWmw7eehLdxzzkR3HHZmBreqJGLJrVw3Bbb,kubeapi,kubeapi
- path: "/opt/custom-env.sh"
  permissions: '0755'
  owner: root
  content: |
    #!/bin/bash
    echo "MASTER_IP=$private_ipv4" > /opt/environment
    echo "NODE_NUM=$(echo $private_ipv4 | awk -F  '.' '{print $4}')" >> /opt/environment
    echo "POD_NETWORK=10.1.0.0/16" >> /opt/environment
    echo "POD_SUBNET=10.1.$(echo $private_ipv4 | awk -F  '.' '{print $4}').0/24" >> /opt/environment
    echo "MGMT_IP=10.1.$(echo $private_ipv4 | awk -F  '.' '{print $4}').2" >> /opt/environment
    echo "API_TOKEN=MXhAQm29G3TLu5NBcumwQvKHyNkZ6XWmw7eehLdxzzkR3HHZmBreqJGLJrVw3Bbb" >> /opt/environment
    echo "ETH_GW=$($private_ipv4/24 | sed -E s%[0-9]+/[0-9]+$%%g)1" >> /opt/environment
